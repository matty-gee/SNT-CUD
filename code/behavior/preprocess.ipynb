{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os, glob\n",
    "user = os.path.expanduser('~')\n",
    "sys.path.insert(0, '..')\n",
    "sys.path.insert(0, f'{user}/Dropbox/Projects/social_navigation_task/social_navigation_task')\n",
    "\n",
    "import functools\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import info as snt_info\n",
    "import preprocess as snt_preprc\n",
    "\n",
    "## version info\n",
    "char_roles = ['first', 'second', 'assistant', 'powerful', 'boss', 'neutral']\n",
    "ver_skincolor = [['Brown','Brown','Brown','White','White','Brown'],\n",
    "                 ['White','White','White','Brown','Brown','White'],\n",
    "                 ['Brown','Brown','Brown','White','White','White'],\n",
    "                 ['White','White','White','Brown','Brown','Brown']]\n",
    "ver_gender = [['Woman','Man','Man','Woman','Man','Woman'],\n",
    "              ['Woman','Man','Man','Woman','Man','Woman'],\n",
    "              ['Man','Woman','Woman','Man','Woman','Man'],\n",
    "              ['Man','Woman','Woman','Man','Woman','Man']]\n",
    "\n",
    "# names for dots\n",
    "ver_name = [['Olivia','Peter','Anthony','Newcomb','Hayworth','Kayce'],\n",
    "            ['Olivia','Peter','Anthony','Newcomb','Hayworth','Kayce'],\n",
    "            ['Peter','Olivia','Kayce','Newcomb','Hayworth','Anthony'],\n",
    "            ['Peter','Olivia','Kayce','Newcomb','Hayworth','Anthony']]\n",
    "\n",
    "## directories\n",
    "syn_dir     = '/Volumes/synapse/projects/SocialSpace/Projects/SNT-fmri_CUD'\n",
    "data_dir    = f'{syn_dir}/Data'\n",
    "snt_dir     = f'{data_dir}/SNT'\n",
    "summary_dir = f'{data_dir}/Summary'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All logs appear to be processed\n"
     ]
    }
   ],
   "source": [
    "# parse the logs\n",
    "logs = glob.glob(f'{snt_dir}/logs/*.log')\n",
    "xlsxs = [f for f in glob.glob(f'{snt_dir}/organized/*.xlsx') if '~$' not in f]\n",
    "if len(logs) == len(xlsxs):\n",
    "    print('All logs appear to be processed')\n",
    "else: \n",
    "    print(f'Found {len(logs)} logs')\n",
    "    for log in logs: \n",
    "        snt_preprc.parse_log(log, experimenter='KB', output_timing=True, out_dir=f'{beh_dir}/organized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute the behavioral geometry\n",
    "out_dir = f'{snt_dir}/behavior'\n",
    "xlsxs = [f for f in glob.glob(f'{snt_dir}/organized/*.xlsx') if '~$' not in f]\n",
    "for i, xlsx in enumerate(xlsxs):\n",
    "    sub_id = xlsx.split('snt_')[1].split('.xlsx')[0]\n",
    "    fname = f'{out_dir}/snt_{sub_id}_behavior.xlsx'\n",
    "    if not os.path.isfile(fname):\n",
    "        print(f'Computing behavior for {sub_id}', r='/n')\n",
    "        snt_preprc.compute_behavior(xlsx, out_dir=out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing 80 of 80\r"
     ]
    }
   ],
   "source": [
    "# summarize the xlsx\n",
    "xlsxs = [f for f in glob.glob(f'{snt_dir}/behavior/*.xlsx') if '~$' not in f]\n",
    "snt_preprc.summarize_behavior(xlsxs, out_dir=summary_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questionnaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sr_df = pd.read_csv(glob.glob(data_dir + '/Questionnaires/CURRENT*')[0])\n",
    "\n",
    "# # exclude cols (phi, timestamps, others)\n",
    "# phi = ['name','phi','phonenum', 'timestamp', 'complet']\n",
    "# cols = sr_df.columns\n",
    "# cols_incl = list(cols[[not any(phi in c for phi in phi) for c in cols]])\n",
    "# sr_df = sr_df[cols_incl]\n",
    "\n",
    "# # sub ids\n",
    "# sub_list = [sub.replace('P','') for sub in sr_df['record_id'].values]\n",
    "# sub_list[np.where(np.array(sub_list)=='21002_20013')[0][0]] = '21002'\n",
    "# sr_df['record_id'] = sub_list\n",
    "# sr_df.rename(columns={'record_id':'sub_id'}, inplace=True)\n",
    "\n",
    "# # sni \n",
    "# num_ppl = ((sr_df['sni_marital_status'] == 1) * 1).values\n",
    "# network_div = ((sr_df['sni_marital_status'] == 1) * 1).values\n",
    "# other_sni = sr_df[['sni_numbr_children_contact','sni_relatives_talk_to_biweekly',\n",
    "#                     'sni_friends_contact_biweekly','sni_religious_affiliation_contact_biweekly',\n",
    "#                     'sni_students_teachers_biweekly_contact','sni_coworkers_biweekly_contact',\n",
    "#                     'sni_neighboors_biweekly_contact','sni_volunteers_biweekly_contact',\n",
    "#                     'sni_other_volunteer_grp_member_biweekly_contact','sni_total_nmbr_other_volunteers_biweekly_contact']]\n",
    "\n",
    "# num_ppl = num_ppl + np.sum(np.nan_to_num(other_sni.values), 1)\n",
    "# network_div = network_div + np.sum((np.nan_to_num(other_sni.values) > 0) * 1, 1)\n",
    "\n",
    "# parents_sni = sr_df[['sni_in_laws_talk_biweekly','sni_parents_contact_biweekly']]\n",
    "# both = (parents_sni == 'both') * 2\n",
    "# mother = (parents_sni == 'mother') * 1\n",
    "# father = (parents_sni == 'father') * 1\n",
    "\n",
    "# sr_df['sni_num_ppl'] = num_ppl + np.sum(both.values + mother.values + father.values, 1)\n",
    "# sr_df['sni_network_div'] = network_div + np.sum(both.values>0, 1) + np.sum(mother.values>0, 1) + np.sum(father.values>0, 1)\n",
    "\n",
    "# sr_df.to_excel(data_dir + '/Summary/Questionnaires_n' + str(len(sr_df)) + '.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory processing completed\n"
     ]
    }
   ],
   "source": [
    "mem_df = pd.read_excel(glob.glob(f'{data_dir}/Summary/SNT-memory*_raw.xlsx')[0])\n",
    "\n",
    "FV = ['Newcomb','Newcomb','Hayworth','Kayce','Jessica','Anthony','Jessica','Chris',\n",
    "      'Kayce','Newcomb','Hayworth','Jessica','Kayce','Anthony','Anthony','Kayce',\n",
    "      'Chris','Newcomb','Anthony','Chris','Kayce','Chris','Newcomb',\n",
    "      'Jessica','Hayworth','Jessica','Hayworth','Anthony','Chris', 'Hayworth']\n",
    "MV = ['Newcomb','Newcomb','Hayworth','Anthony','Chris','Kayce','Chris','Jessica','Anthony','Newcomb',\n",
    "      'Hayworth','Chris','Anthony','Kayce','Kayce','Anthony','Jessica','Newcomb','Kayce','Jessica',\n",
    "      'Anthony','Jessica','Newcomb','Chris','Hayworth','Chris','Hayworth','Kayce','Jessica','Hayworth']\n",
    "roles = ['powerful','powerful','boss','neutral','first','assistant','first','second','neutral','powerful',\n",
    "         'boss','first','neutral','assistant','assistant','neutral','second','powerful','assistant','second',\n",
    "         'neutral','second','powerful','first','boss','first','boss','assistant','second','boss']\n",
    "ques = [f'memory_{r+1}_{role}' for r,role in enumerate(roles)]\n",
    "\n",
    "dfs = []\n",
    "for s,sub in mem_df.iterrows():\n",
    "    v = sub.Task_ver\n",
    "    if v in [1,2]: ans = np.array([x.lower() for x in FV])\n",
    "    else:          ans = np.array([x.lower() for x in MV])\n",
    "    resps = np.array([x.lower() for x in list(sub.values[2:])])\n",
    "    correct = list((resps==ans) * 1)\n",
    "    acc = np.mean(correct)\n",
    "    \n",
    "    df = pd.DataFrame([sub.Sub_id, v, acc] + correct).T\n",
    "    df.columns = ['sub_id','task_version','memory_mean'] + ques\n",
    "    dfs.append(df)\n",
    "mem_df = pd.concat(dfs)\n",
    "mem_df.to_excel(f'{data_dir}/Summary/SNT-memory_n{len(mem_df)}_processed.xlsx')\n",
    "print('Memory processing completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80 dots jpgs\n",
      "Dots appear to be processed\n"
     ]
    }
   ],
   "source": [
    "# parse the dots\n",
    "dots_dir = f'{data_dir}/Dots'\n",
    "jpgs = glob.glob(f'{dots_dir}/Dots*jpg')\n",
    "print(f'Found {len(jpgs)} dots jpgs')\n",
    "if os.path.exists(f'{data_dir}/Summary/SNT-dots_n{len(jpgs)}.xlsx'):\n",
    "    print('Dots appear to be processed')\n",
    "else:\n",
    "    dfs = []\n",
    "    for j, jpg in enumerate(jpgs):\n",
    "        print(f'{j} {jpg}', end=\"\\r\")\n",
    "        try:\n",
    "            sub_id = jpg.split('Dots_')[1].split('.jpg')[0]\n",
    "            df = define_char_coords(load_image(jpg))[1]\n",
    "            df.insert(0, 'sub_id', sub_id)\n",
    "            dfs.append(df)\n",
    "        except:\n",
    "            print(f'ERROR: {fname}')\n",
    "\n",
    "    coords_df = pd.concat(dfs)\n",
    "    coords_df.to_excel(f'{data_dir}/Summary/SNT-dots_n{len(coords_df)}.xlsx', index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fMRI QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75 confound files\n",
      "18004\n",
      "18006\n",
      "18007\n",
      "18010\n",
      "18015\n",
      "18017\n",
      "19004\n",
      "19005\n",
      "19007\n",
      "19009\n",
      "19012\n",
      "19014\n",
      "19016\n",
      "19017\n",
      "19025\n",
      "19027\n",
      "19028\n",
      "19032\n",
      "19042\n",
      "19045\n",
      "19051\n",
      "19052\n",
      "19053\n",
      "19056\n",
      "19057\n",
      "19059\n",
      "20001\n",
      "20006\n",
      "20007\n",
      "20010\n",
      "21002\n",
      "21004\n",
      "21013\n",
      "21014\n",
      "21020\n",
      "21021\n",
      "22001\n",
      "22002\n",
      "75\r"
     ]
    }
   ],
   "source": [
    "tsv_fnames = glob.glob(f'{syn_dir}/QC/cfd_files/*.tsv')\n",
    "tsv_fnames.sort()\n",
    "\n",
    "outname    = f'{summary_dir}/SNT-fMRI_QC_n{len(tsv_fnames)}.xlsx'\n",
    "overwrite  = True\n",
    "cfd_cols   = ['framewise_displacement', 'rmsd', 'std_dvars']\n",
    "# -- dvars: rate of change of BOLD signal across the entire brain at each volume\n",
    "\n",
    "if (os.path.exists(outname)) & (not overwrite):\n",
    "    print('fMRI QC is already summarized')\n",
    "else:\n",
    "    print(f'Found {len(tsv_fnames)} confound files')\n",
    "    dfs = []\n",
    "    for i, fname in enumerate(tsv_fnames):\n",
    "        \n",
    "        print(i+1, end=\"\\r\")\n",
    "        \n",
    "        with open(fname) as f: \n",
    "            cfds = [r for r in csv.reader(f, delimiter=\"\\t\", quotechar='\"')] # load\n",
    "        \n",
    "        sub_id = int(fname.split('sub-P')[-1].split('_')[0])\n",
    "        cfd_df = pd.DataFrame(cfds[1:], columns=cfds[0])\n",
    "        cfd_df.replace(to_replace='n/a', value=np.nan, inplace=True)\n",
    "        cfd_df = cfd_df.astype(float)  \n",
    "        \n",
    "        ## mean and max of confounds\n",
    "        means  = np.nanmean(cfd_df.loc[:, cfd_cols], 0)\n",
    "        maxes  = np.nanmax(cfd_df.loc[:, cfd_cols], 0)\n",
    "\n",
    "        ## find outlier volumes\n",
    "        # framewise displacement > 1 voxel (2.1mm)\n",
    "        fd_onevox   = cfd_df['framewise_displacement'] > 2.1\n",
    "        fd_onevox_n = np.sum(fd_onevox) \n",
    "        if fd_onevox_n>0:\n",
    "            print(sub_id)\n",
    "        try:  \n",
    "            fd_onevox_vols = list(np.where(fd_onevox==1)[0])\n",
    "        except:                \n",
    "            fd_onevox_vols = [np.nan] # none\n",
    "\n",
    "        # fmripreps outliers: motion or intensity spikes\n",
    "        # -- defaults are FD > 0.5 mm or DVARS > 1.5 (seems fairly conservative)\n",
    "        # -- these columns could be used as regressors in first level model\n",
    "        try: \n",
    "            cols = [c for c in cfd_df.columns if 'motion_outlier' in c]\n",
    "            outlier_vols = [np.where(cfd_df[c]==1)[0][0] for c in cols]\n",
    "        except: \n",
    "            outlier_vols = [np.nan] # none \n",
    "\n",
    "        # % of decision volumes that are outliers\n",
    "        outlier_decision   = np.round(len([v for v in outlier_vols if v in decision_epochs])/len(decision_epochs), 2) \n",
    "        fd_onevox_decision = np.round(len([v for v in fd_onevox_vols if v in decision_epochs])/len(decision_epochs), 2) \n",
    "\n",
    "        dfs.append(pd.DataFrame([sub_id] + list(means) + list(maxes) + [outlier_decision, fd_onevox_decision]).T)\n",
    "    \n",
    "df = pd.concat(dfs)\n",
    "df.columns = ['sub_id','fd_mean','rmsd_mean','std_dvars_mean','fd_max','rmsd_max','std_dvars_max','fmriprep_outlier_proportion','fd_outlier_proportion']\n",
    "df = df[['sub_id','fd_mean','fd_max','rmsd_mean','rmsd_max','std_dvars_mean','std_dvars_max','fmriprep_outlier_proportion','fd_outlier_proportion']] # reorganize\n",
    "df.to_excel(outname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# summarize all quality control\n",
    "\n",
    "info = pd.read_excel(f'{syn_dir}/participants_info.xlsx')\n",
    "qc_cols = ['sub_id', 'dx', 'memory_mean', 'missed_trials', 'rt_mean','fd_mean','fd_max',\n",
    "           'rmsd_mean','rmsd_max','std_dvars_mean','std_dvars_max', 'fmriprep_outlier_proportion','fd_outlier_proportion']\n",
    "qc_df = summary_df[qc_cols]\n",
    "\n",
    "qc_df.insert(3, 'memory_incl', (qc_df['memory_mean'] > 0.20) * 1)\n",
    "qc_df.insert(5, 'missed_trials_incl', (qc_df['missed_trials'] < 10) * 1)\n",
    "qc_df.insert(15, 'fd_incl', (qc_df['fd_outlier_proportion'] < 0.05) * 1)\n",
    "qc_df = info.merge(qc_df, on=['sub_id', 'dx'], how='outer')\n",
    "inclusion = qc_df['other_incl'] * qc_df['memory_incl'] * qc_df['missed_trials_incl'] * qc_df['fd_incl']\n",
    "qc_df.insert(0, 'inclusion', inclusion)\n",
    "disp(qc_df.head(4))\n",
    "\n",
    "qc_df.to_excel(f'{syn_dir}/participants_qc_n{len(qc_df)}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for fname in ['SNT-behavior_n*', 'SNT-memory*processed*',\n",
    "              'SNT-fMRI_QC*', 'Questionnaires_*', 'SNT-task_versions.xlsx']:\n",
    "    df = pd.read_excel(glob.glob(f'{summary_dir}/{fname}')[0])\n",
    "    df.sort_values(by='sub_id', inplace=True)\n",
    "    df['sub_id'] = df['sub_id'].astype(int)\n",
    "    dfs.append(df)\n",
    "summary_df = functools.reduce(lambda x, y: pd.merge(x, y, on = 'sub_id'), dfs)\n",
    "first_cols = ['sub_id', 'dx']\n",
    "summary_df = summary_df[first_cols + [c for c in summary_df if c not in first_cols]]\n",
    "del summary_df['version']\n",
    "\n",
    "# task version info\n",
    "dots_df = pd.read_excel(glob.glob(f'{summary_dir}/SNT-dots*')[0])\n",
    "dots_df = summary_df[['sub_id', 'task_version']].merge(dots_df, on='sub_id')\n",
    "\n",
    "sub_dfs = []\n",
    "for s, sub in dots_df.iterrows():\n",
    "    \n",
    "    v = sub.task_version.astype(int) - 1\n",
    "    for n, name in enumerate(ver_name[v]):\n",
    "        sub.rename(index={name + '_affil':'dots_affil_' + char_roles[n]}, inplace=True)\n",
    "        sub.rename(index={name + '_power':'dots_power_' + char_roles[n]}, inplace=True)\n",
    "    gender   = pd.DataFrame(np.array(ver_gender[v]).reshape(1,-1),    columns=[c + '_gender' for c in char_roles])\n",
    "    skintone = pd.DataFrame(np.array(ver_skincolor[v]).reshape(1,-1), columns=[c + '_skincolor' for c in char_roles])\n",
    "    sub_dfs.append(sub.to_frame().T)\n",
    "    \n",
    "summary_df = summary_df.merge(pd.concat(sub_dfs), on=['sub_id','task_version'])\n",
    "summary_df.to_excel(Path(f'{summary_dir}/All-data_summary_n{len(summary_df)}.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
